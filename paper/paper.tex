\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}

\title{Geneus: An Enigma of Organism Intelligence}

\author{Jason Dong, Yidi Huang, Mark Lalor, Andrew Tarnoff, Hung Vu \\
Case Western Reserve University\\
10900 Euclid Ave, Cleveland, OH 44106\\
{\tt\small jwd67@case.edu, yxh597@case.edu, mwl58@case.edu, art81@case.edu, hdv4@case.edu}}

\maketitle

\begin{abstract}
   This will be teh abstract which will be wonderfully descriptive yet succinct.
\end{abstract}

\section{Introduction}

This is the introduction \cite{Alpher02}

\section{Background}

This is the background \cite{Alpher04}

\section{Data}

\begin{figure}
  \includegraphics[width=\linewidth]{data.png}
  \caption{Visualization of the data-processing pipeline.}
  \label{fig:data}
\end{figure}

A flowchart of the data collection and processing pipeline can be seen in Figure \ref{fig:data}. The process involves collecting data from Wikipedia and then processing it into representations that will be used as inputs for the deep learning model.

\subsection{Scrape index pages}
The first step is to creates lists of species articles which is done by supplying manually-found species index pages using \texttt{species\_url\_scraper.py} which then scrapes these articles to output lists of Wikipedia article URLs.

\subsection{Scrape species articles} The next step consumes these lists and scrapes the Wikipedia pages themselves to extract the scientific classification as well as the text content from each of the sections.


\section{Methods}

Our classifier design can be seen in Figure \ref{fig:classifier_overview}. We decided to try two different natural language processing methods to generate representations of the data. The first was a simple keyword extractor followed by a pre-trained word embedding model. The other was BERT, the Bidirectional Encoder Representations from Transformers model by Google. These vectorized representations could then be easily fed into a feed forward model for training. For testing, we widtheld some of the testing data for cross validation and we attempted to write our own paragraphs to see if the model could predict the taxonomy we were thinking of. 
\begin{figure*}
  \includegraphics[width=\linewidth]{classifier_overview.png}
  \caption{Flowchart of our classifier.}
  \label{fig:classifier_overview}
\end{figure*}

\subsection{Keyword Extraction}
We decide to use YAKE, Yet Another Keyword Extractor by Campos et al. Using PKE, the Python Keyphrase Extraction module, we tested several keyword extractors and chose YAKE for its simplicity and proven success. We fed the pretrained model our wikipedia texts and it returned the top 20 keywords and their relevancy scores. 
\begin{figure*}
  \includegraphics[width=\linewidth]{keywords_example.png}
  \caption{Examples of keywords extracted from the lion, tiger, and bluefin tuna wikipedia pages.}
  \label{fig:keywords_example}
\end{figure*}

To vectorize our keywords, we used the pre-trained word embeddings model, GloVe or Global Vectors for Word Representation. The model was pretrained on 2014 wikipedia data which made it suitable for representing our input. In testing, we discovered some words in our data had not been seen by GloVe and we were unable to vectorize it. We decided to remove those words from our training data because it rarely occurred and the pretrained model was able to generate vectors for most of our keywords. A possible alternative would be to train our own GloVe model based on the data we have. 

After vectorizing the keywords, we used a weighted sum of the vectors based on their relevancy scores. Our first attempt to directly sum the vectors resulted in vectors that were all very similar to each other. We attribute this to some common words that are irrelevant to species taxonomy pushing the vectors in a common direction since most wikipedia descriptions use similar language. Our weighted sum approach seemed to overcome this. We multiplied the keyword vectors by its relevancy scores scaled. In other words, the sum of the relevancy scores equalled 1. As a result of this however, our representation varied from the GloVe representation and applying the word embeddings map in the other direction gave irrelevant results. However, from testing different animal species, we saw a good correlation between similar species. As an example, using Euclidean distance as our distance metric, we found that the Lion vector was close to the Tiger vector, but both were far from Bluefin Tuna. This result led us to believe that we have successfully created a representation that stores the characteristics associated with different species. 


\section{Experiments}

These are the experiments.

\section{Results}

These are the results.

\section{Conclusions}

This is wonderful.

{\small
\bibliographystyle{ieee}
\bibliography{bibliography}
}

\end{document}
