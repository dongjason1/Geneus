{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Geneus_Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yLPRjP9qB3rQ",
        "colab": {}
      },
      "source": [
        "#%tensorflow_version 2.x\n",
        "\n",
        "\"\"\"\n",
        "CONNECTING TO LOCAL RUNTIME:\n",
        "\n",
        "jupyter notebook \\\n",
        "  --NotebookApp.allow_origin='https://colab.research.google.com' \\\n",
        "  --port=8888 \\\n",
        "  --NotebookApp.port_retries=0\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import random\n",
        "import os\n",
        "import json\n",
        "\n",
        "from keras.models import Model,Sequential\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.optimizers import adam\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense,Flatten,Input,Reshape\n",
        "from keras.utils import plot_model\n",
        "\n",
        "import random\n",
        "\n",
        "tf.compat.v1.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSYV4dKUETCc",
        "colab_type": "text"
      },
      "source": [
        "# Initialize \"wandb\" Logging to a New Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EREmF2iJESoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wandb\n",
        "#wandb.init(project=\"geneus\", magic=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cCuhszKzCPI4"
      },
      "source": [
        "# Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3QzEoyDPCM-F",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "RUNNING WITH TEST DATA\n",
        "\n",
        "with open('~/Desktop/Geneus/input_datas/dummy/dummy__data.json') as f:\n",
        "    data = json.load(f)\n",
        "    \n",
        "with open('~Desktop/Geneus/input_datas/dummy/dummy__labels.json') as f:\n",
        "    labels = json.load(f)\n",
        "    \n",
        "dataTF = tf.convert_to_tensor(data, dtype=tf.int16)    \n",
        "labelsTF = tf.convert_to_tensor(labels, dtype=tf.int16)\n",
        "\"\"\"\n",
        "\n",
        "def loadJsonIntoTF(path, dataFileNames, labelFileNames, returnTF=True, testing_data_ratio=0.2):\n",
        "  allData = []\n",
        "  allLabels = []\n",
        "  for idx in range(len(dataFileNames)):\n",
        "    try:\n",
        "      with open(path + dataFileNames[idx]) as f:\n",
        "          allData += json.load(f)\n",
        "    except:\n",
        "      print(\"File not found: \" + path + dataFileNames[idx])\n",
        "    \n",
        "    try:\n",
        "      with open(path + labelFileNames[idx]) as f:\n",
        "          allLabels += json.load(f)\n",
        "    except:\n",
        "      print(\"File not found: \" + path + dataFileNames[idx])\n",
        "\n",
        "  #Split into training and testing data\n",
        "  numSamples = max(len(allData), len(allLabels))\n",
        "  splitIdx = int(numSamples*(1 - testing_data_ratio))\n",
        "\n",
        "  allDataNp = np.array(allData)\n",
        "  allLabelsNp = np.array(allLabels)\n",
        "\n",
        "  #Shuffles indices\n",
        "  indices = list(np.random.permutation(numSamples))\n",
        "  training_idx, test_idx = indices[:splitIdx], indices[splitIdx:]\n",
        "\n",
        "  #Training and testing dataset\n",
        "  data_train, data_test = allDataNp[training_idx], allDataNp[test_idx]\n",
        "  labels_train, labels_test = allLabelsNp[training_idx], allLabelsNp[test_idx]\n",
        "\n",
        "  if returnTF:\n",
        "    dataTF_train = tf.convert_to_tensor(data_train, dtype=tf.float32)  \n",
        "    dataTF_test = tf.convert_to_tensor(data_test, dtype=tf.float32)    \n",
        "    labelsTF_train = tf.convert_to_tensor(labels_train, dtype=tf.int16)\n",
        "    labelsTF_test = tf.convert_to_tensor(labels_test, dtype=tf.int16)\n",
        "    return (dataTF_train, dataTF_test, labelsTF_train, labelsTF_test)\n",
        "  else:\n",
        "    return (data_train, data_test, labels_train, labels_test)\n",
        "\n",
        "\n",
        "path = '/home/andrew/Desktop/Geneus/input_datas/'\n",
        "dataFileNames = [\"keyword1_data.json\",\"keyword2_data.json\",\"keyword3_data.json\",\"keyword4_data.json\",\"keyword5_data.json\", \"keyword6_data.json\",\"keyword_data.json\"]\n",
        "labelFileNames = [\"keyword1_labels.json\",\"keyword2_labels.json\",\"keyword3_labels.json\",\"keyword4_labels.json\",\"keyword5_labels.json\", \"keyword6_labels.json\",\"keyword_labels.json\"]\n",
        "\n",
        "pathB = '/home/andrew/Desktop/bert_data/'\n",
        "dataFileNamesB = [\"bert_data.json\"]\n",
        "labelFileNamesB = [\"bert_labels.json\"]\n",
        "\n",
        "pathPhylumLabels = '/home/andrew/Desktop/Geneus/input_datas/'\n",
        "phylumLabelNames = ['class_labels.json']\n",
        "\n",
        "(dataTF_train, dataTF_test, labelsTF_train, labelsTF_test) = loadJsonIntoTF(path, dataFileNames, labelFileNames)\n",
        "(dataTFB_train, dataTFB_test, labelsTFB_train, labelsTFB_test) = loadJsonIntoTF(pathB, dataFileNamesB, labelFileNamesB)\n",
        "\n",
        "#Getting phylum labels\n",
        "(_, _, phylum_labels_train, phylum_labels_test) = loadJsonIntoTF(pathPhylumLabels, phylumLabelNames, phylumLabelNames, False)\n",
        "\n",
        "phylum_labels_train = phylum_labels_train[:, 3:14]\n",
        "phylum_labels_test = phylum_labels_test[:, 3:14]\n",
        "\n",
        "phylum_labelsTF_train = tf.convert_to_tensor(phylum_labels_train, dtype=tf.int16)\n",
        "phylum_labelsTF_test = tf.convert_to_tensor(phylum_labels_test, dtype=tf.int16)\n",
        "\n",
        "#Printout TF to verify correct shapes and types\n",
        "print(dataTF_train)\n",
        "print(dataTF_test)\n",
        "print(labelsTF_train)\n",
        "print(labelsTF_test)\n",
        "print(\"\\n\")\n",
        "print(dataTFB_train)\n",
        "print(dataTFB_test)\n",
        "print(labelsTFB_train)\n",
        "print(labelsTFB_test)\n",
        "print(\"\\n\")\n",
        "print(phylum_labelsTF_train)\n",
        "print(phylum_labelsTF_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCaRkJR7XoMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(phylum_labels_train.shape)\n",
        "#for col in range(11):\n",
        "#  indices = np.where(phylum_labels_train[:, col] == 1) \n",
        "#  print(len(indices[0]))\n",
        "\n",
        "#print(\"\\n\\n\")\n",
        "\n",
        "#print(phylum_labels_test.shape)\n",
        "#for col in range(11):\n",
        "#  indices = np.where(phylum_labels_test[:, col] == 1) \n",
        "#  print(len(indices[0]))\n",
        "\n",
        "\n",
        "#(_, _, labels_train, labels_test) = loadJsonIntoTF(path, dataFileNames, labelFileNames, False)\n",
        "#(_, _, labelsB_train, labelsB_test) = loadJsonIntoTF(pathB, dataFileNamesB, labelFileNamesB, False)\n",
        "#for col in range(3):\n",
        "#  indices = np.where(labels_train[:, col] == 1) \n",
        "#  indices1 = np.where(labelsB_train[:, col] == 1) \n",
        "#  indices2 = np.where(labels_test[:, col] == 1) \n",
        "#  indices3 = np.where(labelsB_test[:, col] == 1) \n",
        "#  print(len(indices[0]))\n",
        "#  print(len(indices1[0]))\n",
        "#  print(len(indices2[0]))\n",
        "#  print(len(indices3[0]))\n",
        "#  print(\"\\n\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ltpZN5v-CTmj"
      },
      "source": [
        "# Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "id-B0EsKCW0C",
        "colab": {}
      },
      "source": [
        "def make_bert_model(inputSize, outputSize, name=\"BERT_Model\"):\n",
        "  #Input Shape is (None, 1024)\n",
        "  #Output Shape is (None, 3)\n",
        "  model = tf.keras.Sequential(name = name)\n",
        "\n",
        "  #Model Params\n",
        "  layerSizes = [32, outputSize]\n",
        "  activations = ['relu', 'softmax']\n",
        "\n",
        "  assert len(layerSizes) == len(activations)\n",
        "  for idx, numNeurons in enumerate(layerSizes):\n",
        "    name = 'Dense_Layer_' + str(idx+1)\n",
        "    if idx == 0:\n",
        "      model.add(layers.Dense(numNeurons, input_dim=inputSize, activation=activations[idx], name=name))\n",
        "    else:\n",
        "      model.add(layers.Dense(numNeurons, activation=activations[idx], name=name))\n",
        "\n",
        "  return model\n",
        "\n",
        "def make_keywords_model(inputSize, outputSize, name=\"Keywords_Model\"):\n",
        "  #Input Shape is (None, 300)\n",
        "  #Output Shape is (None, 3)\n",
        "  model = tf.keras.Sequential(name = name)\n",
        "\n",
        "  #Model Params\n",
        "  layerSizes = [32, outputSize]\n",
        "  activations = ['relu', 'softmax']\n",
        "\n",
        "  assert len(layerSizes) == len(activations)\n",
        "  for idx, numNeurons in enumerate(layerSizes):\n",
        "    name = 'Dense_Layer_' + str(idx+1)\n",
        "    if idx == 0:\n",
        "      model.add(layers.Dense(numNeurons, input_dim=inputSize, activation=activations[idx], name=name))\n",
        "    else:\n",
        "      model.add(layers.Dense(numNeurons, activation=activations[idx], name=name))\n",
        "\n",
        "  return model\n",
        "\n",
        "#Print Model Summaries\n",
        "bertModel = make_bert_model(1024, 3)\n",
        "print(bertModel.summary())\n",
        "\n",
        "kwModel = make_keywords_model(300, 3)\n",
        "print(kwModel.summary())\n",
        "\n",
        "bertPhylumModel = make_bert_model(1024, 11, name=\"BERT_Model_Phylum\")\n",
        "print(bertPhylumModel.summary())\n",
        "\n",
        "kwPhylumModel = make_keywords_model(300, 11, name=\"Keywords_Model_Phylum\")\n",
        "print(kwPhylumModel.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hAxL4qoGCiDy"
      },
      "source": [
        "# Compile and Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7LvCEo-KCkiB",
        "colab": {}
      },
      "source": [
        "#Compile the Models\n",
        "bertPhylumModel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "kwPhylumModel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#Train Model\n",
        "wandb.init(project=\"geneus\", magic=True)\n",
        "bertPhylumModel.fit(dataTFB_train, phylum_labelsTF_train, epochs=100, batch_size=64, validation_data=(dataTFB_test, phylum_labelsTF_test))\n",
        "bertPhylumModel.save(os.path.join(wandb.run.dir, \"bertModel.h5\"))\n",
        "\n",
        "wandb.init(project=\"geneus\", magic=True)\n",
        "kwPhylumModel.fit(dataTF_train, phylum_labelsTF_train, epochs=100, batch_size=64, validation_data=(dataTF_test, phylum_labelsTF_test))\n",
        "kwPhylumModel.save(os.path.join(wandb.run.dir, \"kwModel.h5\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxlIBHQQH5Ag",
        "colab_type": "text"
      },
      "source": [
        "# Load model Weights from a Defined \"wandb\" Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQzqRG6iH4ke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load in the test data\n",
        "(_, bertTest_dataTF_test, _, bertTest_labelsTF_test) = loadJsonIntoTF('/home/andrew/Desktop/Geneus/input_datas/non_wiki/', [\"bert_data.json\"], [\"bert_labels.json\"], testing_data_ratio=1.0)\n",
        "(_, test_dataTF_test, _, test_labelsTF_test) = loadJsonIntoTF('/home/andrew/Desktop/Geneus/input_datas/non_wiki/', [\"keyword_data.json\"], [\"keyword_labels.json\"], testing_data_ratio=1.0)\n",
        "\n",
        "#pathToKwWeightFile = \"/home/andrew/Desktop/Geneus/wandb/Keywords_Run1/kwModel.h5\"\n",
        "#pathToBertWeightFile = \"/home/andrew/Desktop/Geneus/wandb/Bert_Run1/bertModel.h5\"\n",
        "\n",
        "#print(bertTest_labelsTF_test)\n",
        "#print(test_labelsTF_test)\n",
        "#print(\"\\n\")\n",
        "#print(bertTest_dataTF_test)\n",
        "#print(test_dataTF_test)\n",
        "\n",
        "#Get the weights file\n",
        "#kwWeightFile = wandb.restore('kwModel.h5', run_path=\"/wandb/Keywords_Run1/\")\n",
        "#bertWeightFile = wandb.restore('bertModel.h5', run_path=\"/wandb/Bert_Run1/\")\n",
        "\n",
        "#Load weights file into model\n",
        "kwModel_loaded = make_keywords_model(300, 3)\n",
        "kwModel_loaded.load_weights(\"/home/andrew/Desktop/Geneus/wandb/Keywords_Run1/kwModel.h5\")\n",
        "bertModel_loaded = make_keywords_model(1024, 3)\n",
        "bertModel_loaded.load_weights(\"/home/andrew/Desktop/Geneus/wandb/Bert_Run1/bertModel.h5\")\n",
        "\n",
        "#Predict results on testing dataset (for Proof of Concept)\n",
        "print(kwModel_loaded.predict(test_dataTF_test))\n",
        "print(tf.math.argmax(test_labelsTF_test))\n",
        "print(\"\\n\\n\")\n",
        "print(bertModel_loaded.predict(bertTest_dataTF_test))\n",
        "print(tf.math.argmax(bertTest_labelsTF_test))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}